#查看 上海链家（https://sh.lianjia.com/zufang/）的网页结构，找到房源信息所在位置，
#利用 bs4 解析包含房源信息的网页，得到每一个房源的详细信息，将抓取的信息保存至csv


import requests
import csv
from bs4 import BeautifulSoup


def main():
    rows = []
    headers = {'user-agent': '你的user-agent信息',
               'cookie': '你的cookie信息'}
    for i in range(1,5):
        try:
            req = requests.get('https://sh.lianjia.com/zufang/pg%d/#contentList' %i, headers = headers)
        except:
            print("获取网页信息失败")
            continue
        web_data = req.text
        #print(web_data)
        soup = BeautifulSoup(web_data,"lxml")
        for j in soup.find_all("div",class_ = "content__list--item--main"):
            row = []
            titles = j.find("p", class_ = "content__list--item--title twoline")
            data_title = titles.get_text().strip()
            #print(data_title)
            row.append(data_title)
            des = j.find("p", class_ = "content__list--item--des")
            data_des = des.get_text().split("/")
            des_1 = data_des[0].strip()
            des_2 = data_des[1].strip()
            des_3 = data_des[2].strip()
            des_4 = data_des[3].strip()
            temp = [des_1, des_2, des_3, des_4]
            all_des = '/'.join(temp)
            row.append(all_des)
            times = j.find("p",class_ = "content__list--item--time oneline")
            data_time = times.get_text().strip()
            row.append(data_time)
            details = j.find("p",class_ = "content__list--item--bottom oneline")
            detail = details.get_text()
            new_detail = ""
            for items in detail.split():
                new_detail = new_detail + items +"/"
            row.append(new_detail.strip("/"))
            data_price = j.find("span",class_="content__list--item-price").get_text()
            row.append(data_price)
            rows.append(row)
    with open('zufang.csv',"a+",encoding='utf-8') as f:
        f_csv = csv.writer(f)
        f_csv.writerows(rows)        

if __name__ == '__main__':
    main()
